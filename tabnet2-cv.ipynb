{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d705b5",
   "metadata": {},
   "source": [
    "### pytorch-tabnet\n",
    "* 5 fold cv\n",
    "* best result for single model alg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12050242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d856e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "FIGSIZE = (9, 6)\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "FILE_TRAIN = \"train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1755604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for adding features\n",
    "def add_features(df):\n",
    "    new_df = df.copy()\n",
    "    new_df['datetime'] = pd.to_datetime(new_df['datetime'])\n",
    "\n",
    "    # this way I add 3 engineered features\n",
    "    new_df['hour'] = new_df['datetime'].dt.hour\n",
    "    new_df['year'] = new_df['datetime'].dt.year\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7aa2093",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = pd.read_csv(FILE_TRAIN)\n",
    "\n",
    "# feature engineering\n",
    "data_added = add_features(data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df84022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutte le colonne: 14\n",
      "Colonne ignorate: 4\n",
      "target: 1\n",
      "Colonne cat: 6\n",
      "Colonne num: 3\n",
      "Num. features 9\n"
     ]
    }
   ],
   "source": [
    "all_columns = data_added.columns\n",
    "\n",
    "# colonne da ignorare\n",
    "# atemp and temp are strongly correlated (0.98) taking only one\n",
    "del_columns = ['datetime', 'casual', 'registered', 'temp']\n",
    "\n",
    "TARGET = \"count\"\n",
    "\n",
    "cat_cols = ['season', 'holiday','workingday', 'weather', 'hour', 'year']\n",
    "\n",
    "num_cols = list(set(all_columns) - set([TARGET]) - set(del_columns) - set(cat_cols))\n",
    "\n",
    "features = sorted(cat_cols + num_cols)\n",
    "\n",
    "print('Tutte le colonne:', len(all_columns))\n",
    "print('Colonne ignorate:', len(del_columns))\n",
    "print('target:', len([TARGET]))\n",
    "print('Colonne cat:', len(cat_cols))\n",
    "print('Colonne num:', len(num_cols))\n",
    "print('Num. features', len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48bbb4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_used = data_added.drop(del_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87978a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "season\n",
      "holiday\n",
      "workingday\n",
      "weather\n",
      "hour\n",
      "year\n"
     ]
    }
   ],
   "source": [
    "# this part is required for TabNet\n",
    "# categorical_columns = cat_cols\n",
    "categorical_dims =  {}\n",
    "# save label encoder for predictions\n",
    "vet_lenc = []\n",
    "\n",
    "for col in cat_cols:\n",
    "    # print(col, data_used[col].nunique(), data_used[col].unique())\n",
    "    print(col)\n",
    "    l_enc = LabelEncoder()\n",
    "    data_used[col] = l_enc.fit_transform(data_used[col].values)\n",
    "    vet_lenc.append(l_enc)\n",
    "    categorical_dims[col] = len(l_enc.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6faf737",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_idxs = [ i for i, f in enumerate(features) if f in cat_cols]\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in cat_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34533276",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93c481b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing fold: 1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 64512.08286| val_0_rmse: 201.36648| val_0_rmsle: 3.71803 |  0:00:01s\n",
      "epoch 50 | loss: 3015.46704| val_0_rmse: 51.37288| val_0_rmsle: 0.34843 |  0:00:50s\n",
      "epoch 100| loss: 2562.25618| val_0_rmse: 50.92724| val_0_rmsle: 0.44655 |  0:01:38s\n",
      "epoch 150| loss: 2425.86345| val_0_rmse: 51.12546| val_0_rmsle: 0.33278 |  0:02:26s\n",
      "epoch 200| loss: 2400.93228| val_0_rmse: 48.65974| val_0_rmsle: 0.34055 |  0:03:14s\n",
      "\n",
      "Early stopping occurred at epoch 201 with best_epoch = 101 and best_val_0_rmsle = 0.23017\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Processing fold: 2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 64353.03053| val_0_rmse: 195.85575| val_0_rmsle: 1.98202 |  0:00:01s\n",
      "epoch 50 | loss: 3121.24997| val_0_rmse: 52.79271| val_0_rmsle: 0.33041 |  0:00:51s\n",
      "epoch 100| loss: 2678.63233| val_0_rmse: 51.7785 | val_0_rmsle: 0.23772 |  0:01:39s\n",
      "epoch 150| loss: 2189.3605| val_0_rmse: 49.63096| val_0_rmsle: 0.23635 |  0:02:27s\n",
      "epoch 200| loss: 2288.40273| val_0_rmse: 46.03836| val_0_rmsle: 0.25574 |  0:03:15s\n",
      "\n",
      "Early stopping occurred at epoch 240 with best_epoch = 140 and best_val_0_rmsle = 0.18169\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Processing fold: 3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 64789.3294| val_0_rmse: 232.85534| val_0_rmsle: 6.16344 |  0:00:00s\n",
      "epoch 50 | loss: 3234.79025| val_0_rmse: 59.18273| val_0_rmsle: 0.2956  |  0:00:49s\n",
      "epoch 100| loss: 2893.65886| val_0_rmse: 57.42908| val_0_rmsle: 0.30422 |  0:01:38s\n",
      "epoch 150| loss: 2542.75525| val_0_rmse: 55.76312| val_0_rmsle: 0.36845 |  0:02:26s\n",
      "epoch 200| loss: 2356.25124| val_0_rmse: 55.59392| val_0_rmsle: 0.26465 |  0:03:14s\n",
      "epoch 250| loss: 2168.79996| val_0_rmse: 53.0374 | val_0_rmsle: 0.22386 |  0:04:03s\n",
      "epoch 300| loss: 2127.99743| val_0_rmse: 74.77704| val_0_rmsle: 0.52835 |  0:04:52s\n",
      "epoch 350| loss: 2025.33733| val_0_rmse: 51.12245| val_0_rmsle: 0.24764 |  0:05:40s\n",
      "\n",
      "Early stopping occurred at epoch 389 with best_epoch = 289 and best_val_0_rmsle = 0.21107\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Processing fold: 4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 64949.53979| val_0_rmse: 212.46626| val_0_rmsle: 2.71569 |  0:00:00s\n",
      "epoch 50 | loss: 3170.65281| val_0_rmse: 60.59472| val_0_rmsle: 0.27934 |  0:00:49s\n",
      "epoch 100| loss: 2906.58216| val_0_rmse: 52.51135| val_0_rmsle: 0.34188 |  0:01:38s\n",
      "epoch 150| loss: 2456.10965| val_0_rmse: 50.88053| val_0_rmsle: 0.24805 |  0:02:26s\n",
      "\n",
      "Early stopping occurred at epoch 190 with best_epoch = 90 and best_val_0_rmsle = 0.21316\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Processing fold: 5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 63567.65134| val_0_rmse: 244.5002| val_0_rmsle: 3.44552 |  0:00:00s\n",
      "epoch 50 | loss: 3396.4694| val_0_rmse: 63.01641| val_0_rmsle: 0.29507 |  0:00:50s\n",
      "epoch 100| loss: 2947.00168| val_0_rmse: 56.07414| val_0_rmsle: 0.28511 |  0:01:39s\n",
      "epoch 150| loss: 2591.3368| val_0_rmse: 53.29463| val_0_rmsle: 0.27801 |  0:02:27s\n",
      "epoch 200| loss: 2394.61202| val_0_rmse: 52.61695| val_0_rmsle: 0.27592 |  0:03:17s\n",
      "epoch 250| loss: 2298.75627| val_0_rmse: 50.4798 | val_0_rmsle: 0.26859 |  0:04:06s\n",
      "epoch 300| loss: 2306.04512| val_0_rmse: 125.84839| val_0_rmsle: 0.44852 |  0:04:54s\n",
      "epoch 350| loss: 2239.28215| val_0_rmse: 52.60576| val_0_rmsle: 0.26048 |  0:05:45s\n",
      "epoch 400| loss: 2165.10298| val_0_rmse: 51.09443| val_0_rmsle: 0.32637 |  0:06:35s\n",
      "epoch 450| loss: 1954.50748| val_0_rmse: 50.56821| val_0_rmsle: 0.27415 |  0:07:24s\n",
      "epoch 500| loss: 1955.34313| val_0_rmse: 50.885  | val_0_rmsle: 0.25592 |  0:08:12s\n",
      "\n",
      "Early stopping occurred at epoch 532 with best_epoch = 432 and best_val_0_rmsle = 0.21981\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Processing fold: 6\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 64043.96134| val_0_rmse: 231.23148| val_0_rmsle: 3.27712 |  0:00:00s\n",
      "epoch 50 | loss: 3307.47264| val_0_rmse: 56.50085| val_0_rmsle: 0.2184  |  0:00:49s\n",
      "epoch 100| loss: 2860.82538| val_0_rmse: 56.49175| val_0_rmsle: 0.2878  |  0:01:38s\n",
      "\n",
      "Early stopping occurred at epoch 138 with best_epoch = 38 and best_val_0_rmsle = 0.20104\n",
      "Best weights from best epoch are automatically used!\n",
      "\n",
      "Processing fold: 7\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 64448.55866| val_0_rmse: 211.57579| val_0_rmsle: 3.02357 |  0:00:01s\n",
      "epoch 50 | loss: 3449.25856| val_0_rmse: 56.02811| val_0_rmsle: 0.36383 |  0:00:50s\n",
      "epoch 100| loss: 3089.72522| val_0_rmse: 52.93211| val_0_rmsle: 0.35482 |  0:01:39s\n",
      "epoch 150| loss: 2792.00421| val_0_rmse: 53.57137| val_0_rmsle: 0.3491  |  0:02:27s\n",
      "epoch 200| loss: 2675.95085| val_0_rmse: 57.18548| val_0_rmsle: 0.31671 |  0:03:16s\n",
      "epoch 250| loss: 2442.71826| val_0_rmse: 49.39411| val_0_rmsle: 0.28397 |  0:04:05s\n",
      "\n",
      "Early stopping occurred at epoch 275 with best_epoch = 175 and best_val_0_rmsle = 0.25448\n",
      "Best weights from best epoch are automatically used!\n",
      "CPU times: user 31min 41s, sys: 7.9 s, total: 31min 49s\n",
      "Wall time: 32min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "FOLDS = 7\n",
    "\n",
    "skf = KFold(n_splits = FOLDS, shuffle=True, random_state = SEED)\n",
    "\n",
    "# provato sembra meglio n_steps = 2\n",
    "# forse perchè va in overfitting\n",
    "params = {'n_steps':2,\n",
    "          'cat_dims':cat_dims,\n",
    "          'cat_idxs':cat_idxs,\n",
    "          'verbose':50\n",
    "         }\n",
    "\n",
    "# we will save here all the results from FOLDS\n",
    "best_models = []\n",
    "\n",
    "EPOCHS = 1000\n",
    "PATIENCE = 100\n",
    "\n",
    "i = 1\n",
    "for train_idx, valid_idx in skf.split(data_used):\n",
    "    print()\n",
    "    print('Processing fold:', i)\n",
    "    \n",
    "    data_train = data_used.iloc[train_idx]\n",
    "    data_valid = data_used.iloc[valid_idx]\n",
    "    \n",
    "    x_train = data_train[features].values\n",
    "    y_train = data_train[TARGET].values\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "    x_valid = data_valid[features].values\n",
    "    y_valid = data_valid[TARGET].values\n",
    "    y_valid = y_valid.reshape(-1, 1)\n",
    "    \n",
    "    model = TabNetRegressor(**params)\n",
    "\n",
    "    # provo a cercare direttamente best su rmsle\n",
    "    model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], eval_metric=['rmse', 'rmsle'], \n",
    "              max_epochs=EPOCHS, patience=PATIENCE, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    best_models.append(model)\n",
    "    \n",
    "    # next iteration\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7952b664",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7c65ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_orig = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11527578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add engineered features\n",
    "test_orig = add_features(test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80bd6e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "season\n",
      "holiday\n",
      "workingday\n",
      "weather\n",
      "hour\n",
      "year\n"
     ]
    }
   ],
   "source": [
    "# code categorical\n",
    "for i, col in enumerate(cat_cols):\n",
    "    print(col)\n",
    "    l_enc = vet_lenc[i]\n",
    "    test_orig[col] = l_enc.transform(test_orig[col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5485db64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions from model 1\n",
      "\n",
      "Predictions from model 2\n",
      "\n",
      "Predictions from model 3\n",
      "\n",
      "Predictions from model 4\n",
      "\n",
      "Predictions from model 5\n",
      "\n",
      "Predictions from model 6\n",
      "\n",
      "Predictions from model 7\n"
     ]
    }
   ],
   "source": [
    "x_test = test_orig[features].values\n",
    "\n",
    "avg_score = np.zeros((x_test.shape[0], 1))\n",
    "                     \n",
    "for i,model in enumerate(best_models):\n",
    "    print()\n",
    "    print('Predictions from model', i+1)\n",
    "    \n",
    "    score_test = model.predict(x_test)\n",
    "    \n",
    "    avg_score += score_test/float(FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35de5292",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(\"sampleSubmission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23261502",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub[\"count\"] = avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baa53d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace negative with zero\n",
    "condition = df_sub[\"count\"] < 0\n",
    "\n",
    "df_sub.loc[condition, \"count\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f40e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_SUB = \"submission34.csv\"\n",
    "\n",
    "df_sub.to_csv(FILE_SUB, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6c18dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 244k/244k [00:01<00:00, 146kB/s]\n",
      "Successfully submitted to Bike Sharing Demand"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c \"bike-sharing-demand\" -f $FILE_SUB -m \"sub34 tabnet cv 7-folds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba01b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
