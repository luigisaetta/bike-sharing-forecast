{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa826f87",
   "metadata": {},
   "source": [
    "### Tuning HP: XGBoost and Optuna\n",
    "* adopting sklearn cross validate\n",
    "* using rmsle as metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b02034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# the GBM used\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# to encode categoricals\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# see utils.py\n",
    "from utils import add_features, rmsle, train_encoders, apply_encoders \n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8915e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "\n",
    "FILE_TRAIN = \"train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce1f6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train dataset\n",
    "data_orig = pd.read_csv(FILE_TRAIN)\n",
    "\n",
    "#\n",
    "# Data preparation, feature engineering\n",
    "#\n",
    "\n",
    "# add features (hour, year) extracted form timestamp\n",
    "data_extended = add_features(data_orig)\n",
    "\n",
    "# ok, we will treat as categorical: holiday, hour, season, weather, workingday, year\n",
    "all_columns = data_extended.columns\n",
    "\n",
    "# cols to be ignored\n",
    "# atemp and temp are strongly correlated (0.98) we're taking only one (atemp)\n",
    "del_columns = ['datetime', 'casual', 'registered', 'temp']\n",
    "\n",
    "TARGET = \"count\"\n",
    "cat_cols = ['season', 'holiday','workingday', 'weather', 'hour', 'year']\n",
    "num_cols = list(set(all_columns) - set([TARGET]) - set(del_columns) - set(cat_cols))\n",
    "features = sorted(cat_cols + num_cols)\n",
    "\n",
    "# drop ignored columns\n",
    "data_used = data_extended.drop(del_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad0f053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train for coding: season \n",
      "train for coding: weather \n",
      "train for coding: year \n",
      "\n",
      "Coding: season \n",
      "Coding: weather \n",
      "Coding: year \n"
     ]
    }
   ],
   "source": [
    "# Code categorical columns (only season, weather, year)\n",
    "le_list = train_encoders(data_used)\n",
    "\n",
    "# coding\n",
    "data_used = apply_encoders(data_used, le_list)\n",
    "\n",
    "# reorder columns, move count at the end\n",
    "data_used = data_used[features + [TARGET]]\n",
    "\n",
    "# define indexes for cat_cols\n",
    "# not using now, but can be useful in future\n",
    "cat_columns_idxs = [i for i, col in enumerate(features) if col in cat_cols]\n",
    "\n",
    "# finally we have the train dataset\n",
    "X = data_used[features].values\n",
    "y = data_used[TARGET].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212183d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the HPO session with Optuna\n",
    "FOLDS = 7\n",
    "SEED = 4321\n",
    "\n",
    "N_TRIALS = 60\n",
    "STUDY_NAME = \"gbm11\"\n",
    "\n",
    "# ranges\n",
    "LR_LOW = 1e-4\n",
    "LR_HIGH = 1e-2\n",
    "DEPTH_LOW = 5\n",
    "DEPTH_HIGH = 10\n",
    "N_ITER_LIST = [3000, 3500, 4000, 4500, 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b66070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Here we define what we do using Optuna\n",
    "#\n",
    "def objective(trial):\n",
    "    \n",
    "    # tuning on these parameters\n",
    "    # names are implementation (diff for xg etc)\n",
    "    dict_params = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", low=LR_LOW, high=LR_HIGH),\n",
    "        \"max_depth\" : trial.suggest_int(\"max_depth\", DEPTH_LOW, DEPTH_HIGH),\n",
    "        \"num_boost_round\": trial.suggest_categorical(\"num_boost_round\", N_ITER_LIST)\n",
    "    }\n",
    "    \n",
    "    # for XGBoost seems I have to pass esplicitely n_estimators\n",
    "    regr = xgb.XGBRegressor(n_estimators = dict_params[\"num_boost_round\"],\n",
    "                                                       **dict_params)\n",
    "    \n",
    "    # using rmsle for scoring\n",
    "    # greater is better is Flase because it is an error measure\n",
    "    # then make_scorer sign-flip and therefore we will maximize it to get the best\n",
    "    scorer = make_scorer(rmsle, greater_is_better=False)\n",
    "    \n",
    "    scores = cross_validate(regr, X, y, cv=FOLDS, scoring=scorer)\n",
    "    \n",
    "    avg_test_score = round(np.mean(scores['test_score']), 4)\n",
    "        \n",
    "    return avg_test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeec9af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-03-09 15:44:16,912]\u001b[0m A new study created in memory with name: gbm11\u001b[0m\n",
      "\u001b[32m[I 2022-03-09 15:46:53,155]\u001b[0m Trial 0 finished with value: -0.5475 and parameters: {'learning_rate': 0.00029979319507730237, 'max_depth': 7, 'num_boost_round': 4000}. Best is trial 0 with value: -0.5475.\u001b[0m\n",
      "\u001b[32m[I 2022-03-09 15:50:35,766]\u001b[0m Trial 1 finished with value: -1.0371 and parameters: {'learning_rate': 0.00011875324061696935, 'max_depth': 9, 'num_boost_round': 3500}. Best is trial 0 with value: -0.5475.\u001b[0m\n",
      "\u001b[32m[I 2022-03-09 15:53:15,324]\u001b[0m Trial 2 finished with value: -0.5084 and parameters: {'learning_rate': 0.005280182147906995, 'max_depth': 8, 'num_boost_round': 3500}. Best is trial 2 with value: -0.5084.\u001b[0m\n",
      "\u001b[32m[I 2022-03-09 15:56:27,442]\u001b[0m Trial 3 finished with value: -0.8908 and parameters: {'learning_rate': 0.00010266352067178083, 'max_depth': 7, 'num_boost_round': 5000}. Best is trial 2 with value: -0.5084.\u001b[0m\n",
      "\u001b[32m[I 2022-03-09 15:58:57,673]\u001b[0m Trial 4 finished with value: -0.5279 and parameters: {'learning_rate': 0.0003127595252060066, 'max_depth': 6, 'num_boost_round': 5000}. Best is trial 2 with value: -0.5084.\u001b[0m\n",
      "KeyboardInterrupt: "
     ]
    }
   ],
   "source": [
    "# launch Optuna Study\n",
    "\n",
    "study = optuna.create_study(study_name=STUDY_NAME , direction=\"maximize\")\n",
    "\n",
    "study.optimize(objective, n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1803c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameters are:')\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7909246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize trials as an ordered Pandas df\n",
    "df = study.trials_dataframe()\n",
    "\n",
    "result_df = df[df['state'] == 'COMPLETE'].sort_values(by=['value'], ascending=False)\n",
    "\n",
    "# best on top\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560dee1",
   "metadata": {},
   "source": [
    "### Train the model with best params on train set and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = xgb.XGBRegressor(n_estimators = study.best_params['num_boost_round'], \n",
    "                         **study.best_params)\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260abe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"xgboost.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5be28b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
